{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78baf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp2\\ipykernel_6612\\1477637772.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import powerlaw\n",
    "import re\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f16d2",
   "metadata": {},
   "source": [
    "For building the network, the relationships between the characters will be used, as created in the data building notebook. Moreover, in order to identify and keep only the most important characters, the characters dataframe is also loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb0a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the locations of the dataframes to be used in the analysis, ie. characters' names and relationships\n",
    "tot_relationships_url = 'https://raw.githubusercontent.com/gpanagioto/projects_socialgraphs22/main/Project2/Relationships/total_relationships.csv'\n",
    "characters_df_url = 'https://raw.githubusercontent.com/gpanagioto/projects_socialgraphs22/main/Project2/characters_df.csv'\n",
    "\n",
    "# Load the dataframes\n",
    "relationships_pd = pd.read_csv(tot_relationships_url, index_col=0)\n",
    "characters_df = pd.read_csv(characters_df_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4e8c0",
   "metadata": {},
   "source": [
    "Now we can create a network by [using the relationship dataframe as edgelist](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) and the weight as an edge attribute to show relationship strength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef692af",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(relationships_pd,\n",
    "                           source='source',\n",
    "                           target='target',\n",
    "                           edge_attr='weight',\n",
    "                           create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ac79c",
   "metadata": {},
   "source": [
    "Initial network statistics, number of nodes, edges, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6c3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2728\n",
      "Number of edges: 17324\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423ae54",
   "metadata": {},
   "source": [
    "### Connected component  \n",
    "\n",
    "Since the graph is undirected, we use the function [`connected_components`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d51f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of connected components: 19\n"
     ]
    }
   ],
   "source": [
    "Gcc = list( nx.connected_components(G))\n",
    "Gcc = sorted(Gcc, key=len, reverse=True)\n",
    "print(f\"Number of connected components: {len(Gcc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e393a8",
   "metadata": {},
   "source": [
    "A number of connected components exist. Their size in regards to nodes is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a80f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0: 2685\n",
      "Component 1: 5\n",
      "Component 2: 4\n",
      "Component 3: 3\n",
      "Component 4: 3\n",
      "Component 5: 2\n",
      "Component 6: 2\n",
      "Component 7: 2\n",
      "Component 8: 2\n",
      "Component 9: 2\n",
      "Component 10: 2\n",
      "Component 11: 2\n",
      "Component 12: 2\n",
      "Component 13: 2\n",
      "Component 14: 2\n",
      "Component 15: 2\n",
      "Component 16: 2\n",
      "Component 17: 2\n",
      "Component 18: 2\n"
     ]
    }
   ],
   "source": [
    "for i,  comp in enumerate(Gcc):\n",
    "    print(f\"Component {i}: {len(comp)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09e4fa",
   "metadata": {},
   "source": [
    "Since only the first component holds the vast majority of nodes, this is the network that is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c567fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G.subgraph(Gcc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc38a6",
   "metadata": {},
   "source": [
    "### Centrality measures to decrease the number of nodes to the most important ones\n",
    "\n",
    "This is a relatively large network, especially when trying to analyze interactions between characters. Not all of them are central to the plot. Some might be ancillary to the plot, or recurring. Some others might only be present to a couple or one episode. Obviously the latter are not valuable in the following analysis, since they are also bound to have low text content in their dialogues. Finally, there is the case of aggregated characters, like `Man`, `Woman`, `Officer` etc. While these generic names could have been removed during the character dataframe building phase, they are included to avoid biasing the analysis with prior knowledge of the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344df235",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_centr = nx.eigenvector_centrality(G)\n",
    "closeness_centr = nx.closeness_centrality(G)\n",
    "degree_centr = nx.degree_centrality(G)\n",
    "betweenness = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': list(betweenness.keys()),\n",
    "        'betweenness':list(betweenness.values()),\n",
    "        'eigenvector':list(eigen_centr.values()),\n",
    "        'degree': list(degree_centr.values()),\n",
    "        'closeness':list(closeness_centr.values()),\n",
    "       }\n",
    "\n",
    "centralities_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d0cd5",
   "metadata": {},
   "source": [
    "The centrality measure used to remove some characters is betweenness centrality. Eigenvector centrality can favor cahracters not as important, but that have connected to major characters. First, the characters with betweenness centrality equal to $0$ are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_reduced_df = centralities_df.loc[centralities_df.betweenness > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618637df",
   "metadata": {},
   "source": [
    "Afterwards, the characters can be fiurther reduced by using selecting charactres that belong to the upper percentiles of the same centrality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 50 # Percentile over which the characters are selected\n",
    "\n",
    "cols = characters_reduced_df.columns\n",
    "centralities_perc =  np.percentile( characters_reduced_df[cols[1]], percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d74845",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = np.histogram( np.array(characters_reduced_df.betweenness.to_list()), bins = 200 )\n",
    "\n",
    "plt.bar(edges[:-1], hist, width=0.0005)\n",
    "plt.plot( [centralities_perc, centralities_perc], [0, hist.max()], 'r--' )\n",
    "plt.yscale('log')\n",
    "plt.title(f'Betweenness centrality distribution\\n for positive betweenness values\\n{percentile}\\'th percentile: {np.round(centralities_perc, 5)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_centralities_df = characters_reduced_df[ (characters_reduced_df[cols[1]] > centralities_perc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a04d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_centralities_df.sort_values('betweenness', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b549dc",
   "metadata": {},
   "source": [
    "Now save the reduced characteres list for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd868523",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_centralities_df.reset_index(inplace=True, drop=True)\n",
    "reduced_centralities_df[['name']].to_csv('reduced_characters_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26385892",
   "metadata": {},
   "source": [
    "### With the reduced characters, build the relationships again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_path = Path.cwd() / 'Relationships'\n",
    "scripts_dir = Path.cwd() / 'Scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = reduced_centralities_df.name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_interactions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_in_text(text, characters_list):\n",
    "\n",
    "    pattern = r'\\n(.+)\\s:'\n",
    "    matches = re.findall(pattern, text)\n",
    "    chars = [nm.strip() for nm in matches]\n",
    "    \n",
    "    char_list = []\n",
    "    for character in chars:\n",
    "        if character in characters_list:\n",
    "            char_list.append(character)\n",
    "    \n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationship_dict(char_list):\n",
    "    relationship_dict_list = []\n",
    "    for i, el in enumerate(char_list[:-1]):\n",
    "        for character in char_list[i+1:]:\n",
    "            if not character == el:\n",
    "                relationship_dict_list.append({ 'source':el, 'target':character })\n",
    "    return relationship_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern. Compile it to be faster since there are many files\n",
    "pattern = r\"[+]{2}\\n([^+]+)[+]{2}\"\n",
    "prog = re.compile(pattern)\n",
    "\n",
    "total_relationships_dict_list = []\n",
    "\n",
    "for file_ in tqdm(scripts_dir.glob('**/*.txt')):\n",
    "        \n",
    "    episode_relationship_dict_list = []\n",
    "    season_nr = file_.as_posix().split('/')[-2]\n",
    "    fname = file_.as_posix().split('/')[-1] \n",
    "    fname = fname.split('.')[0]\n",
    "    \n",
    "    # Create a folder for each season and save the csv of the relationships in there for each episode\n",
    "    season_path = relationships_path / f\"{season_nr}\"\n",
    "    season_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(file_, 'r', encoding='utf-8') as f:\n",
    "        test_txt = f.read()\n",
    "            \n",
    "    \n",
    "    # Use regex to find the text between the pluses\n",
    "    results = prog.findall(test_txt)\n",
    "    for result in results:\n",
    "        # Get the list of characters in this scene\n",
    "        chars_in_part = get_characters_in_text(result, names)\n",
    "        # If there are more than 1 characters in the list, create a relationship between them and\n",
    "        # append to the corresponding lists\n",
    "\n",
    "        if len(chars_in_part)>1:\n",
    "            rel_lst = create_relationship_dict(chars_in_part)                \n",
    "            episode_relationship_dict_list += rel_lst\n",
    "            total_relationships_dict_list += rel_lst\n",
    "    \n",
    "    # For this episode, create now a dataframe from the episode relationships\n",
    "    episode_rel_df = pd.DataFrame(episode_relationship_dict_list)\n",
    "    \n",
    "    # I have duplicates. I can add them as weights.\n",
    "    # But first to have all of the same names on the same column\n",
    "    # I want for a specific pair of source and target\n",
    "    # the source to be always on the same column of the dataframe\n",
    "    episode_rel_df = pd.DataFrame( np.sort(episode_rel_df.values, axis=1), columns=episode_rel_df.columns )\n",
    "    \n",
    "    # For the duplicates, we can add them up to form weights on the edges, representing\n",
    "    # how strong the relationship is\n",
    "    episode_rel_df['weight'] = 1 # initialize\n",
    "    try:\n",
    "        episode_rel_df = episode_rel_df.groupby(['source', 'target'], sort=False, as_index=False).sum()\n",
    "        episode_rel_df.to_csv(season_path.as_posix()+'/'+fname+'_reduced'+'.csv')\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        print(season_nr, fname)\n",
    "        print(episode_rel_df)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "# Do the same for the total relationship\n",
    "total_relationships_dict_list = pd.DataFrame(total_relationships_dict_list)\n",
    "total_relationships_dict_list = pd.DataFrame( np.sort(total_relationships_dict_list.values, axis=1), columns=total_relationships_dict_list.columns )\n",
    "total_relationships_dict_list['weight'] = 1\n",
    "total_relationships_dict_list = total_relationships_dict_list.groupby(['source', 'target'], sort=False, as_index=False).sum()\n",
    "total_relationships_dict_list.to_csv(relationships_path.as_posix()+'/'+'total_relationships_reduced'+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb7e76",
   "metadata": {},
   "source": [
    "### Build total scripts for each of the reduced characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46769cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_reduced_texts_path = Path.cwd() / 'Character_Texts' / 'Characters'\n",
    "characters_reduced_texts_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6860df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_path = Path.cwd() / 'Texts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2407b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112dbad1071e42eea36ef11cbd647554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb14fdf873342cca58b37ae16038fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3033 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_txt_dict = {}\n",
    "for file in tqdm(texts_path.glob('*/*/*.txt')):\n",
    "    char_name = file.stem\n",
    "    \n",
    "    with open(file.as_posix(), 'r', encoding='utf-8') as f:\n",
    "        doc = f.read()\n",
    "    \n",
    "    char_txt_dict[char_name] = char_txt_dict.get(char_name, '') + doc + '\\n'\n",
    "    \n",
    "for character in tqdm(list(char_txt_dict.keys())):\n",
    "    with open(characters_reduced_texts_path.as_posix()+f\"/{character}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(char_txt_dict[character])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
