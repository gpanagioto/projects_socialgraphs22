{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacde2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67706e8",
   "metadata": {},
   "source": [
    "In this notebook, an attempt to get only the most important characters from south park, without any prior knowledge regarding the show, will be made.  \n",
    "\n",
    "These attempt are based on network metrics, centrality measures and degree distributions.  \n",
    "\n",
    "We will then dispose of the previous network and create a new one, using only the important nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd3fde",
   "metadata": {},
   "source": [
    "First, we load the data from the characters dataframe and the previously created netowrk, with all characters and nodes form fandom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73edabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "characters_df = pd.read_csv('characters_df_large.csv')\n",
    "\n",
    "# Load network\n",
    "with open('south_park_digraph.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f449c",
   "metadata": {},
   "source": [
    "The main idea for getting rid of the 'noise', or keeping only the most important characters, is to look at the characters' 'connectedness' using different criteria:  \n",
    "- Degree (total, or in and out)\n",
    "- Centrality measures:  \n",
    "  - Closeness  \n",
    "  - Betweenness  \n",
    "  - Eigenvector based.\n",
    "  \n",
    "We will look at all measures, because each one represents different properties. In the ned, we can either normalize and take a weighted average of all the criteria for each character, and keep the top percentile, or use these metrics and extract the chaarcters with a different startegy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b002b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_centrality_sorted(centrality_dict, top_n = None):\n",
    "    a = sorted( centrality_dict.items(), key=lambda x: x[1], reverse=True )\n",
    "    if top_n:\n",
    "        return a[:top_n]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273aee4",
   "metadata": {},
   "source": [
    "#### Betweenness centrality:  \n",
    "\n",
    "A node has high betweenness centrality if many shortest paths between the other nodes in the network pass through it. High betweenness centrality for a node measn that a lot of information passess through this node, so it is bound to play a central role in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9ac22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eric Cartman', 0.2655766515997768),\n",
       " ('Stan Marsh', 0.13896494221466524),\n",
       " ('Kyle Broflovski', 0.10836022860439216),\n",
       " ('Randy Marsh', 0.09806368708331918),\n",
       " ('Butters Stotch', 0.07892490799843789),\n",
       " ('Herbert Garrison', 0.06804477974477509),\n",
       " ('The Boys', 0.06799352133780456),\n",
       " ('Kenny McCormick', 0.05297638908276538),\n",
       " ('The 4th Grade', 0.03806317281053689),\n",
       " ('Jimmy Valmer', 0.02845081381096589)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweenness = nx.betweenness_centrality(G.to_undirected(), normalized=True)\n",
    "return_centrality_sorted(betweenness, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3ab7c",
   "metadata": {},
   "source": [
    "#### Eigenvector centrality:  \n",
    "\n",
    "Takes the neighboring nodes into consideration. A node has high eigenvector centrality if its neighbors have high centrality.  It is a measure of the importance of the node, since the higher the centrality, the more connected to important nodes the node is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9d99ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('McNuggets', 0.14313313731241356),\n",
       " ('Elon Musk', 0.1423357627341641),\n",
       " ('ISIS', 0.14122043095826411),\n",
       " ('PewDiePie', 0.1411205578470931),\n",
       " ('Jeff White', 0.1405343827111894),\n",
       " ('Dan Snyder', 0.1402312422510927),\n",
       " ('NFL Owners and Presidents', 0.1400859319673538),\n",
       " ('Unnamed Interior Designer', 0.1400409547113547),\n",
       " ('Taylor Swift', 0.13996296497869212),\n",
       " ('Possessed Stan Marsh', 0.13992283574187606)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvector = nx.eigenvector_centrality(G.to_undirected())\n",
    "return_centrality_sorted(eigenvector, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a225960",
   "metadata": {},
   "source": [
    "#### Degree centrality:  \n",
    "\n",
    "A measure of the number of nodes connected to the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1d1a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eric Cartman', 0.2622871046228711),\n",
       " ('Stan Marsh', 0.17420924574209246),\n",
       " ('Kyle Broflovski', 0.15766423357664233),\n",
       " ('Butters Stotch', 0.11824817518248176),\n",
       " ('Randy Marsh', 0.11435523114355231),\n",
       " ('Kenny McCormick', 0.09732360097323602),\n",
       " ('The Boys', 0.09099756690997568),\n",
       " ('Herbert Garrison', 0.08564476885644769),\n",
       " ('The 4th Grade', 0.07201946472019465),\n",
       " ('Wendy Testaburger', 0.04720194647201947)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_centr = nx.degree_centrality(G.to_undirected())\n",
    "return_centrality_sorted(degree_centr, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcad801",
   "metadata": {},
   "source": [
    "#### Closeness centrality:  \n",
    "Nodes with high closeness centrality are also close to other nodes. Meaning that they will most probably have many short paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de511da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Eric Cartman', 0.5144044304150723),\n",
       " ('Stan Marsh', 0.4829465684114917),\n",
       " ('Kyle Broflovski', 0.4783676702121729),\n",
       " ('Butters Stotch', 0.45082205107858536),\n",
       " ('Kenny McCormick', 0.44485967273955923),\n",
       " ('Randy Marsh', 0.4443155684431557),\n",
       " ('The Boys', 0.4400101850280088),\n",
       " ('Herbert Garrison', 0.43443244290734767),\n",
       " ('Clyde Donovan', 0.41996810261170414),\n",
       " ('Wendy Testaburger', 0.41678798618531904)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closeness_centr = nx.closeness_centrality(G.to_undirected())\n",
    "return_centrality_sorted(closeness_centr, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf89082",
   "metadata": {},
   "source": [
    "How can we combine the above metrics to pick the most important characters, since for each metric, the top list is not the same?   \n",
    "First of all, we notice that only the eigenvector centrality is very different compared to the other centrality measures. But we cannot ignore it.  \n",
    "\n",
    "An idea is to create an equally weighted mean of each centrality measure for each character.  \n",
    "\n",
    "Another idea comes from the fact that some characters might just appear only once and then be connected to the most important characters for one episode, they might have very high eigenvector centrality. To avoid this, we multiply together all the measures for each character.  \n",
    "\n",
    "Finally, we can find percentiles for each measure and select the characters that are above a certain percentile in all centrality measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5a860",
   "metadata": {},
   "source": [
    "We can create a pandas dataframe where the first column is the character name and then each column holds its cecntrality measure. Finally, in the last column the equally weighted mean is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5308d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': list(betweenness.keys()),\n",
    "        'betweenness':list(betweenness.values()),\n",
    "        'eigenvector':list(eigenvector.values()),\n",
    "        'degree': list(degree_centr.values()),\n",
    "        'closeness':list(closeness_centr.values()),\n",
    "       }\n",
    "\n",
    "centralities_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "308ff095",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = centralities_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14e8abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [50,50, 50,50] # Weigh betweenness more\n",
    "\n",
    "centralities_perc = [ np.percentile( centralities_df[cols[i+1]], percentiles[i]) for i in range(4) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ff9b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_centralities_df = centralities_df[  (centralities_df[cols[1]] > centralities_perc[0])\n",
    "                & (centralities_df[cols[2]] > centralities_perc[1]) \n",
    "                & (centralities_df[cols[3]] > centralities_perc[2]) \n",
    "                & (centralities_df[cols[4]] > centralities_perc[3]) \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a969341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>eigenvector</th>\n",
       "      <th>degree</th>\n",
       "      <th>closeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric Cartman</td>\n",
       "      <td>0.265577</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.262287</td>\n",
       "      <td>0.514404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stan Marsh</td>\n",
       "      <td>0.138965</td>\n",
       "      <td>0.051882</td>\n",
       "      <td>0.174209</td>\n",
       "      <td>0.482947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kyle Broflovski</td>\n",
       "      <td>0.108360</td>\n",
       "      <td>0.047553</td>\n",
       "      <td>0.157664</td>\n",
       "      <td>0.478368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>Randy Marsh</td>\n",
       "      <td>0.098064</td>\n",
       "      <td>0.032506</td>\n",
       "      <td>0.114355</td>\n",
       "      <td>0.444316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Butters Stotch</td>\n",
       "      <td>0.078925</td>\n",
       "      <td>0.026209</td>\n",
       "      <td>0.118248</td>\n",
       "      <td>0.450822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>Unified Atheist League (UAL)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.342129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>Rex</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.362765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.358965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Hackelm Cartman</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.349435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>Thad (\"Insecurity\")</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.313544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name  betweenness  eigenvector    degree  \\\n",
       "0                     Eric Cartman     0.265577     0.078864  0.262287   \n",
       "3                       Stan Marsh     0.138965     0.051882  0.174209   \n",
       "2                  Kyle Broflovski     0.108360     0.047553  0.157664   \n",
       "1545                   Randy Marsh     0.098064     0.032506  0.114355   \n",
       "236                 Butters Stotch     0.078925     0.026209  0.118248   \n",
       "...                            ...          ...          ...       ...   \n",
       "1935  Unified Atheist League (UAL)     0.000011     0.002155  0.002433   \n",
       "1568                           Rex     0.000011     0.004621  0.002920   \n",
       "1105               Mark Zuckerberg     0.000011     0.003944  0.003893   \n",
       "707                Hackelm Cartman     0.000011     0.003489  0.003406   \n",
       "1821           Thad (\"Insecurity\")     0.000010     0.001206  0.001946   \n",
       "\n",
       "      closeness  \n",
       "0      0.514404  \n",
       "3      0.482947  \n",
       "2      0.478368  \n",
       "1545   0.444316  \n",
       "236    0.450822  \n",
       "...         ...  \n",
       "1935   0.342129  \n",
       "1568   0.362765  \n",
       "1105   0.358965  \n",
       "707    0.349435  \n",
       "1821   0.313544  \n",
       "\n",
       "[624 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_centralities_df.sort_values('betweenness', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e12eda",
   "metadata": {},
   "source": [
    "What do we have as a result? Many characters which are indeed central (from someone that has seen the series) but also many characters which are:  \n",
    "- Groups (eg. Coon and friends, The Boys)  \n",
    "- Characters from one specific episode, marked with parentheses  \n",
    "- Characters which have no names and are some other character's relatives. Usually with a `'`  \n",
    "\n",
    "Assuming no prior knowledge of the show, one cannot know in advance if any of these aforementioned character groups should be removed. As a result, we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347654f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fb63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e363e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326530ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03923c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
