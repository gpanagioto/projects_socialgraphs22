{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9052d251",
   "metadata": {},
   "source": [
    "In this script, the data needed for the network analysis of the show South Park are generated. These are:  \n",
    " - The characters\n",
    " - The scripts, split in scenes, for each episode  \n",
    " \n",
    "The analysis is based on relationships between the characters, as they are built between the scenes of each episode. To this end, the following actions are required:  \n",
    " - Get the scripts for each episode from each season. The Wiki Fandom website is used.\n",
    " - From each script, get the characters, what they are saying, as well as the scenes, to be used as separators. This is needed because an episode might be consisting of parallel stories, with different character interactions. We want to create relationships between characters that are in the same scenes, not only because they appear in an episode.\n",
    " \n",
    "From the above gathered data, the unique characters list can be built. Moreover, after some formatting for later use, the scripts are also available to be used for sentiment and other text based analysis.  \n",
    "\n",
    "The character names can be aquired via 2 ways:  \n",
    "1. From the beginning of the dialogue in the script. This is the most obvious way.\n",
    "2. Taking advantage of the wiki website, that lists all the characters in a single episode in the beginning of a script page.  \n",
    "\n",
    "However, this second way uses the full names, as a result further analysis will be required to match the characters to the names in the scripts. As a result, the first method will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4230fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp2\\ipykernel_10368\\3947806153.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247a84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the scripts\n",
    "\n",
    "scripts_dir = Path.cwd() / 'Scripts'\n",
    "scripts_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac63b47",
   "metadata": {},
   "source": [
    "### Part 1: Get the scripts links for all episodes of each season  \n",
    "\n",
    "The goal is to get the source of the wiki page that stores the links. It is much easier to get using the library `bs4`, since only one textbox is present in a source page. The use regular expressions to extract the links for the scripts of each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f08e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_url(link):\n",
    "    '''\n",
    "    Convert a link aquired from scraping to wiki link for the source text.\n",
    "    '''\n",
    "    # Have a base url to append the links to, in order to request the page needed at a time\n",
    "    base_url = 'https://southpark.fandom.com/wiki/'\n",
    "    \n",
    "    # Define an 'api' string, that when appended to a url, it directs to its source\n",
    "    source_api = '?action=edit'\n",
    "    \n",
    "    url = base_url + link + source_api\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5775b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_textbox(url):\n",
    "    '''\n",
    "    Return the text from the (unique) textbox of the source wiki page\n",
    "    '''\n",
    "    raw_html = requests.get(url)\n",
    "    soup = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "    main_table = soup.find_all('textarea', class_='mw-editfont-default')\n",
    "    \n",
    "    try:\n",
    "        text = main_table[0].text\n",
    "        return text\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b06031",
   "metadata": {},
   "source": [
    "Links for each season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92554e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_titles(text, mode='s'):\n",
    "    '''\n",
    "    Return tuple of lists. First element is a list of urls. Second is the title.\n",
    "    mode: 's' for season , 'e' for episodes.\n",
    "    '''\n",
    "    if mode.lower()=='s':\n",
    "        pattern = r'\\|\\[{2}(.+)\\|(.+)\\]{2}'\n",
    "    elif mode.lower() == 'e':\n",
    "        pattern = r'\\\"\\[{2}(.+)\\|(.+)\\]{2}'\n",
    "    else:\n",
    "        print('wrong mode')\n",
    "        return None\n",
    "    \n",
    "    matches = re.findall(pattern, text)\n",
    "    max_elements = len(str(len(matches))) if matches else 0\n",
    "    \n",
    "    links = [] # Separate lists for urls\n",
    "    titles = [] # and titles\n",
    "    for i, match in enumerate(matches):\n",
    "        links.append(link_to_url(match[0].replace(' ', '_').replace('?', '%3F')))\n",
    "        \n",
    "        # Append 0 to the numbering to avoid sorting problems\n",
    "        el_num = '0'*(max_elements-len(str((i+1)))) + str(i+1)\n",
    "        element_title = match[1].replace(' ', '_')\n",
    "        element_title = element_title.replace(\"'\", '_')\n",
    "        element_title = element_title.replace('?', '%3F')\n",
    "        titles.append(el_num+'_'+element_title)\n",
    "    return links, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e3626",
   "metadata": {},
   "source": [
    "From the textbox of source page of each wiki link, two elements can be identified:  \n",
    " - `ScriptScene`: Describes a scene setting  \n",
    " - `ScriptDialog` : Describes the character and what he/she says  \n",
    "\n",
    "Use regular expressions to isolate the content of each of the above elements.  \n",
    "\n",
    "Since these are stored in a text file, the startegy to sepate them later is laid:  \n",
    "- Each scene begins and ends with 3 `+`: +++ Something happens +++\n",
    "- The character's name is followed by a space,  `:`, a space and then what they say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff07147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialog(dialog_textbox):\n",
    "    dialog_pattern = r'\\{\\{ScriptDialog\\|(.+)\\|(.+)\\}\\}'\n",
    "    matches = re.findall(dialog_pattern, dialog_textbox)\n",
    "    \n",
    "    document = ''\n",
    "    for match in matches:\n",
    "        document += match[0] + \" : \"\n",
    "        document += match[1] + '\\n'\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4ed29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_script_textbox(textbox):\n",
    "    # Identify the scenes. Get their content and (start) and ending index\n",
    "    scene_pattern = r'\\{\\{ScriptScene\\|(.+)\\}\\}'\n",
    "    matches  =re.finditer(scene_pattern, textbox)\n",
    "    \n",
    "    scene_separators = '+'*5\n",
    "    \n",
    "    starts = []\n",
    "    ends = []\n",
    "    content = []\n",
    "    for match in matches:\n",
    "        starts.append(match.start())\n",
    "        ends.append(match.end())\n",
    "        content.append(match.group(1))\n",
    "    \n",
    "    \n",
    "    # Take only the parts between scenes. Each episode starts with a scene and ends with dialogue\n",
    "    starts = starts[1:]\n",
    "    ends = ends[0:-1]\n",
    "    \n",
    "    # Start creating the document for this episode\n",
    "    document = ''\n",
    "    document = scene_separators + content[0] + scene_separators + '\\n'\n",
    "    \n",
    "    content_i = 1\n",
    "    for start_i, end_i in zip(ends, starts):\n",
    "        dialog_txt = textbox[start_i: end_i]\n",
    "                \n",
    "        processed_dialog_txt = process_dialog(dialog_txt)\n",
    "        document += processed_dialog_txt\n",
    "        \n",
    "        # Add the next scene text\n",
    "        document += scene_separators + content[content_i] + scene_separators + '\\n'\n",
    "        content_i += 1\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbb6e448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5982786f7e4ec2b0b06a276d8283a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = 'https://southpark.fandom.com/wiki/Portal:Scripts?action=edit'\n",
    "seasons_textbox = url_to_textbox(url)\n",
    "season_urls, season_names = get_links_titles(seasons_textbox)\n",
    "\n",
    "# Create a dataframe to store season names, links and episode title and link\n",
    "links_df = pd.DataFrame({'Season':[], 'Episode':[], 'URL':[]})\n",
    "\n",
    "for i in tqdm(range(len(season_names))):\n",
    "    # Create a dir in the scripts dir for the season\n",
    "    season_path = scripts_dir / season_names[i]\n",
    "    season_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all the episode links and names of the season\n",
    "    episodes_textbox = url_to_textbox(season_urls[i])\n",
    "    episodes_urls, episodes_names = get_links_titles(episodes_textbox, 'e')\n",
    "    \n",
    "    # Create lists for the dataframe update\n",
    "    season_nm = []\n",
    "    episode_nm = []\n",
    "    episode_lnk = []\n",
    "    for j in range(len(episodes_names)):\n",
    "        \n",
    "        # To update dataframe\n",
    "        season_nm.append(season_names[i])\n",
    "        episode_nm.append(episodes_names[j])\n",
    "        episode_lnk.append(episodes_urls[j])\n",
    "        \n",
    "        # Get the raw data for the scripts of this episode\n",
    "        episode_textbox = url_to_textbox(episodes_urls[j])\n",
    "        \n",
    "        # Find scenes and dialogs and add them to a formatted document\n",
    "        document = process_script_textbox(episode_textbox)\n",
    "        \n",
    "        # Save the formatted script in a text file        \n",
    "        with open(season_path.as_posix()+f'/{episodes_names[j]}.txt', 'w', encoding='utf8') as f:\n",
    "            f.write(document)\n",
    "            \n",
    "    temp_df = pd.DataFrame({'Season': season_nm, 'Episode':episode_nm, 'URL':episode_lnk})\n",
    "    links_df = pd.concat([links_df, temp_df])\n",
    "        \n",
    "\n",
    "# Save the created dataframe in a csv file\n",
    "links_df.to_csv('episode_script_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a41e35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a339c38",
   "metadata": {},
   "source": [
    "### Part 2: Get the characters from the documents  \n",
    "\n",
    "Many things can be combined, like that character list and the relationship building between the characters for the network. However, it is better to proceed step by step, since there is no need for performance or running the script real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0fc871fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_characters(formatted_episode_script):\n",
    "    characters_pattern = r\"\\n([\\w\\d\\s.,'-]+)\\s:\"\n",
    "    matches = re.findall(characters_pattern, formatted_episode_script)\n",
    "    \n",
    "    characters = []\n",
    "    for match in matches:\n",
    "        # Remove groups\n",
    "        if len(match.split(','))>1:\n",
    "            continue\n",
    "        if len(match.split('and'))>1:\n",
    "            continue\n",
    "        characters.append(match.strip())\n",
    "    \n",
    "    # Remove duplicates due to introduction\n",
    "    single_name = [character for character in set(characters) if len(character.split())<2]\n",
    "    dual_name = [character for character in set(characters) if len(character.split())>1]\n",
    "    tmp_dual_chars = dual_name[:]\n",
    "    \n",
    "    for character in single_name:\n",
    "        for el in tmp_dual_chars:\n",
    "            if character in el.split():\n",
    "                tmp_dual_chars.remove(el)\n",
    "\n",
    "    characters = single_name[:] + tmp_dual_chars[:]\n",
    "    \n",
    "    \n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba41eb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc18b23e96044bf59d1c7552e474e951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "characters = []\n",
    "for ep_script in tqdm(scripts_dir.glob('**/*.txt')):\n",
    "    with open(ep_script.as_posix(), 'r', encoding='utf8') as f:\n",
    "        document = f.read()\n",
    "    episode_characters = get_episode_characters(document)\n",
    "    characters += episode_characters\n",
    "    \n",
    "# We want unique characters, avoid repetition between episodes\n",
    "characters = list(set(characters))\n",
    "\n",
    "# Save the characters as a pandas dataframe\n",
    "characters_df = pd.DataFrame.from_dict({'name':characters})\n",
    "characters_df.to_csv('characters_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c45d3",
   "metadata": {},
   "source": [
    "### Relationships between characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34c68377",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_path = Path.cwd() / 'Relationships'\n",
    "relationships_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8130b9",
   "metadata": {},
   "source": [
    "We are only interested in the names of the characters here. So we can create a dictionary with keys the full name and values the first name. Or maybe we could create two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9bc5a05f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'first_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3800\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3801\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'first_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp2\\ipykernel_10368\\217207066.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Go with the two lists approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfull_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcharacters_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfirst_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcharacters_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3805\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3806\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3801\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'first_name'"
     ]
    }
   ],
   "source": [
    "# Go with the two lists approach\n",
    "full_names = characters_df.name.to_list()\n",
    "first_names = characters_df['first_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
