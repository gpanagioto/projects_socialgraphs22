{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a36e236",
   "metadata": {},
   "source": [
    "In this script, the data needed for the network analysis of the show South Park are generated. These are:  \n",
    " - The characters\n",
    " - The scripts, split in scenes, for each episode  \n",
    " \n",
    "The analysis is based on relationships between the characters, as they are built between the scenes of each episode. To this end, the following actions are required:  \n",
    " - Get the scripts for each episode from each season. The Wiki Fandom website is used.\n",
    " - From each script, get the characters, what they are saying, as well as the scenes, to be used as separators. This is needed because an episode might be consisting of parallel stories, with different character interactions. We want to create relationships between characters that are in the same scenes, not only because they appear in an episode.\n",
    " \n",
    "From the above gathered data, the unique characters list can be built. Moreover, after some formatting for later use, the scripts are also available to be used for sentiment and other text based analysis.  \n",
    "\n",
    "The character names can be aquired via 2 ways:  \n",
    "1. From the beginning of the dialogue in the script. This is the most obvious way.\n",
    "2. Taking advantage of the wiki website, that lists all the characters in a single episode in the beginning of a script page.  \n",
    "\n",
    "However, this second way uses the full names, as a result further analysis will be required to match the characters to the names in the scripts. As a result, the first method will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "77ae4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fa4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the scripts\n",
    "\n",
    "scripts_dir = Path.cwd() / 'Scripts'\n",
    "scripts_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2463b94",
   "metadata": {},
   "source": [
    "### Part 1: Get the scripts links for all episodes of each season  \n",
    "\n",
    "The goal is to get the source of the wiki page that stores the links. It is much easier to get using the library `bs4`, since only one textbox is present in a source page. The use regular expressions to extract the links for the scripts of each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c7d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_url(link):\n",
    "    '''\n",
    "    Convert a link aquired from scraping to wiki link for the source text.\n",
    "    '''\n",
    "    # Have a base url to append the links to, in order to request the page needed at a time\n",
    "    base_url = 'https://southpark.fandom.com/wiki/'\n",
    "    \n",
    "    # Define an 'api' string, that when appended to a url, it directs to its source\n",
    "    source_api = '?action=edit'\n",
    "    \n",
    "    url = base_url + link + source_api\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddeea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_textbox(url):\n",
    "    '''\n",
    "    Return the text from the (unique) textbox of the source wiki page\n",
    "    '''\n",
    "    raw_html = requests.get(url)\n",
    "    soup = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "    main_table = soup.find_all('textarea', class_='mw-editfont-default')\n",
    "    \n",
    "    try:\n",
    "        text = main_table[0].text\n",
    "        return text\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841235c",
   "metadata": {},
   "source": [
    "Links for each season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb0b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_titles(text, mode='s'):\n",
    "    '''\n",
    "    Return tuple of lists. First element is a list of urls. Second is the title.\n",
    "    mode: 's' for season , 'e' for episodes.\n",
    "    '''\n",
    "    if mode.lower()=='s':\n",
    "        pattern = r'\\|\\[{2}(.+)\\|(.+)\\]{2}'\n",
    "    elif mode.lower() == 'e':\n",
    "        pattern = r'\\\"\\[{2}(.+)\\|(.+)\\]{2}'\n",
    "    else:\n",
    "        print('wrong mode')\n",
    "        return None\n",
    "    \n",
    "    matches = re.findall(pattern, text)\n",
    "    max_elements = len(str(len(matches))) if matches else 0\n",
    "    \n",
    "    links = [] # Separate lists for urls\n",
    "    titles = [] # and titles\n",
    "    for i, match in enumerate(matches):\n",
    "        links.append(link_to_url(match[0].replace(' ', '_').replace('?', '%3F')))\n",
    "        \n",
    "        # Append 0 to the numbering to avoid sorting problems\n",
    "        el_num = '0'*(max_elements-len(str((i+1)))) + str(i+1)\n",
    "        element_title = match[1].replace(' ', '_')\n",
    "        element_title = element_title.replace(\"'\", '_')\n",
    "        element_title = element_title.replace('?', '%3F')\n",
    "        titles.append(el_num+'_'+element_title)\n",
    "    return links, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843ff98",
   "metadata": {},
   "source": [
    "From the textbox of source page of each wiki link, two elements can be identified:  \n",
    " - `ScriptScene`: Describes a scene setting  \n",
    " - `ScriptDialog` : Describes the character and what he/she says  \n",
    "\n",
    "Use regular expressions to isolate the content of each of the above elements.  \n",
    "\n",
    "Since these are stored in a text file, the startegy to sepate them later is laid:  \n",
    "- Each scene begins and ends with 3 `+`: +++ Something happens +++\n",
    "- The character's name is followed by a space,  `:`, a space and then what they say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e76d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dialog(dialog_textbox):\n",
    "    dialog_pattern = r'\\{\\{ScriptDialog\\|(.+)\\|(.+)\\}\\}'\n",
    "    matches = re.findall(dialog_pattern, dialog_textbox)\n",
    "    \n",
    "    document = ''\n",
    "    for match in matches:\n",
    "        document += match[0] + \" : \"\n",
    "        document += match[1] + '\\n'\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5f147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_script_textbox(textbox):\n",
    "    # Identify the scenes. Get their content and (start) and ending index\n",
    "    scene_pattern = r'\\{\\{ScriptScene\\|(.+)\\}\\}'\n",
    "    matches  =re.finditer(scene_pattern, textbox)\n",
    "    \n",
    "    scene_separators = '+'*5\n",
    "    \n",
    "    starts = []\n",
    "    ends = []\n",
    "    content = []\n",
    "    for match in matches:\n",
    "        starts.append(match.start())\n",
    "        ends.append(match.end())\n",
    "        content.append(match.group(1))\n",
    "    \n",
    "    \n",
    "    # Take only the parts between scenes. Each episode starts with a scene and ends with dialogue\n",
    "    starts = starts[1:]\n",
    "    ends = ends[0:-1]\n",
    "    \n",
    "    # Start creating the document for this episode\n",
    "    document = ''\n",
    "    document = scene_separators + content[0] + scene_separators + '\\n'\n",
    "    \n",
    "    content_i = 1\n",
    "    for start_i, end_i in zip(ends, starts):\n",
    "        dialog_txt = textbox[start_i: end_i]\n",
    "                \n",
    "        processed_dialog_txt = process_dialog(dialog_txt)\n",
    "        document += processed_dialog_txt\n",
    "        \n",
    "        # Add the next scene text\n",
    "        document += scene_separators + content[content_i] + scene_separators + '\\n'\n",
    "        content_i += 1\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c2bff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5982786f7e4ec2b0b06a276d8283a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = 'https://southpark.fandom.com/wiki/Portal:Scripts?action=edit'\n",
    "seasons_textbox = url_to_textbox(url)\n",
    "season_urls, season_names = get_links_titles(seasons_textbox)\n",
    "\n",
    "# Create a dataframe to store season names, links and episode title and link\n",
    "links_df = pd.DataFrame({'Season':[], 'Episode':[], 'URL':[]})\n",
    "\n",
    "for i in tqdm(range(len(season_names))):\n",
    "    # Create a dir in the scripts dir for the season\n",
    "    season_path = scripts_dir / season_names[i]\n",
    "    season_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all the episode links and names of the season\n",
    "    episodes_textbox = url_to_textbox(season_urls[i])\n",
    "    episodes_urls, episodes_names = get_links_titles(episodes_textbox, 'e')\n",
    "    \n",
    "    # Create lists for the dataframe update\n",
    "    season_nm = []\n",
    "    episode_nm = []\n",
    "    episode_lnk = []\n",
    "    for j in range(len(episodes_names)):\n",
    "        \n",
    "        # To update dataframe\n",
    "        season_nm.append(season_names[i])\n",
    "        episode_nm.append(episodes_names[j])\n",
    "        episode_lnk.append(episodes_urls[j])\n",
    "        \n",
    "        # Get the raw data for the scripts of this episode\n",
    "        episode_textbox = url_to_textbox(episodes_urls[j])\n",
    "        \n",
    "        # Find scenes and dialogs and add them to a formatted document\n",
    "        document = process_script_textbox(episode_textbox)\n",
    "        \n",
    "        # Save the formatted script in a text file        \n",
    "        with open(season_path.as_posix()+f'/{episodes_names[j]}.txt', 'w', encoding='utf8') as f:\n",
    "            f.write(document)\n",
    "            \n",
    "    temp_df = pd.DataFrame({'Season': season_nm, 'Episode':episode_nm, 'URL':episode_lnk})\n",
    "    links_df = pd.concat([links_df, temp_df])\n",
    "        \n",
    "\n",
    "# Save the created dataframe in a csv file\n",
    "links_df.to_csv('episode_script_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4f436",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b74aeb",
   "metadata": {},
   "source": [
    "### Part 2: Get the characters from the documents  \n",
    "\n",
    "Many things can be combined, like that character list and the relationship building between the characters for the network. However, it is better to proceed step by step, since there is no need for performance or running the script real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e1bc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_characters(formatted_episode_script):\n",
    "    characters_pattern = r\"\\n([\\w\\d\\s.,'-]+)\\s:\"\n",
    "    matches = re.findall(characters_pattern, formatted_episode_script)\n",
    "    \n",
    "    characters = []\n",
    "    for match in matches:\n",
    "        # Remove groups\n",
    "        if len(match.split(','))>1:\n",
    "            continue\n",
    "        if len(match.split('and'))>1:\n",
    "            continue\n",
    "        characters.append(match.strip())\n",
    "    \n",
    "    # Remove duplicates due to introduction\n",
    "    single_name = [character for character in set(characters) if len(character.split())<2]\n",
    "    dual_name = [character for character in set(characters) if len(character.split())>1]\n",
    "    tmp_dual_chars = dual_name[:]\n",
    "    \n",
    "    for character in single_name:\n",
    "        for el in tmp_dual_chars:\n",
    "            if character in el.split():\n",
    "                tmp_dual_chars.remove(el)\n",
    "\n",
    "    characters = single_name[:] + tmp_dual_chars[:]\n",
    "    \n",
    "    \n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80eb3a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc18b23e96044bf59d1c7552e474e951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "characters = []\n",
    "for ep_script in tqdm(scripts_dir.glob('**/*.txt')):\n",
    "    with open(ep_script.as_posix(), 'r', encoding='utf8') as f:\n",
    "        document = f.read()\n",
    "    episode_characters = get_episode_characters(document)\n",
    "    characters += episode_characters\n",
    "    \n",
    "# We want unique characters, avoid repetition between episodes\n",
    "characters = list(set(characters))\n",
    "\n",
    "# Save the characters as a pandas dataframe\n",
    "characters_df = pd.DataFrame.from_dict({'name':characters})\n",
    "characters_df.to_csv('characters_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbfd06",
   "metadata": {},
   "source": [
    "### Relationships between characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3306e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_path = Path.cwd() / 'Relationships'\n",
    "relationships_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd72f2c",
   "metadata": {},
   "source": [
    "We are only interested in the names of the characters here. So we can create a dictionary with keys the full name and values the first name. Or maybe we could create two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "64c9c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go with the two lists approach\n",
    "names = characters_df.name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b79be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to create a data frame from it afterwards\n",
    "# It will be of the form:   'first_char': 'second_char'\n",
    "characters_interactions = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54062533",
   "metadata": {},
   "source": [
    "Define two functions used to search for names and relationships in the scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c3146160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_in_text(text, characters_list):\n",
    "#     txt_list_tmp = [ el.strip() for el in text.split('\\n') if el ]\n",
    "#     char_list = []\n",
    "#     for character in characters_list:\n",
    "#         if character in txt_list_tmp:\n",
    "#             char_list.append(character)\n",
    "    pattern = r'\\n(.+)\\s:'\n",
    "    matches = re.findall(pattern, text)\n",
    "    chars = [nm.strip() for nm in matches]\n",
    "    \n",
    "    char_list = []\n",
    "    for character in chars:\n",
    "        if character in characters_list:\n",
    "            char_list.append(character)\n",
    "    \n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "14715670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationship_dict(char_list):\n",
    "    relationship_dict_list = []\n",
    "    for i, el in enumerate(char_list[:-1]):\n",
    "        for character in char_list[i+1:]:\n",
    "            if not character == el:\n",
    "                relationship_dict_list.append({ 'source':el, 'target':character })\n",
    "    return relationship_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bf9300e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66499e6b3fd04df69f5a37e4e4b92c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a regex pattern. Compile it to be faster since there are many files\n",
    "pattern = r\"[+]{2}\\n([^+]+)[+]{2}\"\n",
    "prog = re.compile(pattern)\n",
    "\n",
    "total_relationships_dict_list = []\n",
    "\n",
    "for file_ in tqdm(scripts_dir.glob('**/*.txt')):\n",
    "        \n",
    "    episode_relationship_dict_list = []\n",
    "    season_nr = file_.as_posix().split('/')[-2]\n",
    "    fname = file_.as_posix().split('/')[-1]\n",
    "    fname = fname.split('.')[0]\n",
    "    \n",
    "    # Create a folder for each season and save the csv of the relationships in there for each episode\n",
    "    season_path = relationships_path / f\"{season_nr}\"\n",
    "    season_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(file_, 'r', encoding='utf-8') as f:\n",
    "        test_txt = f.read()\n",
    "            \n",
    "    \n",
    "    # Use regex to find the text between the pluses\n",
    "    results = prog.findall(test_txt)\n",
    "    for result in results:\n",
    "        # Get the list of characters in this scene\n",
    "        chars_in_part = get_characters_in_text(result, names)\n",
    "        # If there are more than 1 characters in the list, create a relationship between them and\n",
    "        # append to the corresponding lists\n",
    "\n",
    "        if len(chars_in_part)>1:\n",
    "            rel_lst = create_relationship_dict(chars_in_part)                \n",
    "            episode_relationship_dict_list += rel_lst\n",
    "            total_relationships_dict_list += rel_lst\n",
    "    \n",
    "    # For this episode, create now a dataframe from the episode relationships\n",
    "    episode_rel_df = pd.DataFrame(episode_relationship_dict_list)\n",
    "    \n",
    "    # I have duplicates. I can add them as weights.\n",
    "    # But first to have all of the same names on the same column\n",
    "    # I want for a specific pair of source and target\n",
    "    # the source to be always on the same column of the dataframe\n",
    "    episode_rel_df = pd.DataFrame( np.sort(episode_rel_df.values, axis=1), columns=episode_rel_df.columns )\n",
    "    \n",
    "    # For the duplicates, we can add them up to form weights on the edges, representing\n",
    "    # how strong the relationship is\n",
    "    episode_rel_df['weight'] = 1 # initialize\n",
    "    try:\n",
    "        episode_rel_df = episode_rel_df.groupby(['source', 'target'], sort=False, as_index=False).sum()\n",
    "        episode_rel_df.to_csv(season_path.as_posix()+'/'+fname+'.csv')\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        print(season_nr, fname)\n",
    "        print(episode_rel_df)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "# Do the same for the total relationship\n",
    "total_relationships_dict_list = pd.DataFrame(total_relationships_dict_list)\n",
    "total_relationships_dict_list = pd.DataFrame( np.sort(total_relationships_dict_list.values, axis=1), columns=total_relationships_dict_list.columns )\n",
    "total_relationships_dict_list['weight'] = 1\n",
    "total_relationships_dict_list = total_relationships_dict_list.groupby(['source', 'target'], sort=False, as_index=False).sum()\n",
    "total_relationships_dict_list.to_csv(relationships_path.as_posix()+'/'+'total_relationships'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a468f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47965b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413d623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
