{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4cb2cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c8e5a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dir_path):\n",
    "    if not dir_path.is_dir():\n",
    "        os.mkdir(dir_path)\n",
    "        print(f'Created: {dir_path.as_posix()}')\n",
    "    else:\n",
    "        print(f\"{dir_path.as_posix()} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f984cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_script(script_url):\n",
    "    '''\n",
    "    Input is just the html page, without action added.\n",
    "    return the text of the episode script. \n",
    "    Always starts with 5 dashes. And ends with 5 dashes\n",
    "    Scenes are separated by also 5 dashes\n",
    "    '''\n",
    "    raw_html = requests.get(script_url)\n",
    "\n",
    "    soup = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "\n",
    "    # Get the (one) box of the script links information\n",
    "    try:\n",
    "        main_table = soup.find_all('table', class_='headerscontent')[1]\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(script_url)\n",
    "        main_table = soup.find_all('table', class_='headerscontent')     \n",
    "        text = ''\n",
    "        for table in main_table:\n",
    "            mids = main_table.find_all('td', class_='DLborderBOT')\n",
    "            \n",
    "            for mid in mids:\n",
    "                if len(mid.text)<=2:\n",
    "                    text += '-'*5 + '\\n'\n",
    "                text += mid.text + '\\n'\n",
    "            text += '-'*5\n",
    "\n",
    "        return text\n",
    "        \n",
    "    \n",
    "#     for table in main_table[1:]: # From table 1\n",
    "#     rights = table.find_all('td', class_='DLborderRIGHT')\n",
    "    mids = main_table.find_all('td', class_='DLborderBOT')\n",
    "\n",
    "    text = ''\n",
    "    for mid in mids:\n",
    "        if len(mid.text)<=2:\n",
    "            text += '-'*5 + '\\n'\n",
    "        text += mid.text + '\\n'\n",
    "    text += '-'*5\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec38269",
   "metadata": {},
   "source": [
    "Create the directory to store all the scripts in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5c1a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts exists\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd() # So that I know where I start\n",
    "\n",
    "# Set the path of the directory in which the scripts for each season and each episode will be stored in\n",
    "scripts_path = current_dir / 'Scripts'\n",
    "\n",
    "# Create the Scripts dir\n",
    "make_dir(scripts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c131e7",
   "metadata": {},
   "source": [
    "Now I can get the links to all the seasons to search in for episodes and finally scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5266e3e",
   "metadata": {},
   "source": [
    "Url of the [scripts portal](https://southpark.fandom.com/wiki/Portal:Scripts). Lists all the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "529e4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base url. Used to add to it\n",
    "base_url = r'https://southpark.fandom.com/wiki/'\n",
    "\n",
    "# For the 'API', when we want to get the wikisource style page. It is easier to parse\n",
    "action = r'?action=edit' \n",
    "\n",
    "# Create the url to get the seasons links\n",
    "scripts_main_url = base_url+ r'Portal:Scripts' + action\n",
    "raw_scripts_main_html = requests.get(scripts_main_url)\n",
    "soup_season = BeautifulSoup(raw_scripts_main_html.text, 'html.parser')\n",
    "\n",
    "# Get the (one) box of the script links information\n",
    "main_box_seasons = soup_season.find_all('textarea', id='wpTextbox1')[0] # It always has 1 box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "48087487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to get all the seasons links in a list. This pattern will be used\n",
    "# for the seasons links and then the episodes links.\n",
    "# pattern = r'\\|link\\=(.+)\\n'\n",
    "\n",
    "pattern = r\"\\|link\\=([^\\|\\n]+)\\n\"\n",
    "\n",
    "# Create a dictionary for the links. It is a dict of dicts. Keys are season numbers and values are dictionaries with keys episode\n",
    "# and link as value\n",
    "scripts_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bbe15",
   "metadata": {},
   "source": [
    "Now start going inside every season then every episode and finally inside each script. Get its text and store it in a text file in the corresponding folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f8e13bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f17eaab3b84983813414f3e310b74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/1\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/2\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/3\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/4\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/5\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/6\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/7\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/8\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/9\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/10\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/11\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/12\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/13\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/14\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/15\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/16\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/17\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/18\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/19\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/20\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/21\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/22\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/23\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/24\n",
      "Created: C:/Users/user/Documents/DTU/Fall_22_23/Social graphs/g_panag_repo/projects_socialgraphs22/Project/Scripts/25\n"
     ]
    }
   ],
   "source": [
    "season = 1\n",
    "matches_season = re.findall(pattern , main_box_seasons.get_text())\n",
    "for match_season in tqdm(matches_season):\n",
    "    # Get the link and replace space with _. Not essential, automatically done\n",
    "    link_name_seas = match_season.replace(' ', '_')\n",
    "    \n",
    "    # Get the name of the season \n",
    "#     ses_dir_name = link_name.split('/')[-1]\n",
    "    ses_dir_name = str(season)\n",
    "    \n",
    "    # Directory to create\n",
    "    season_dir_path = scripts_path / ses_dir_name\n",
    "    make_dir(season_dir_path)\n",
    "    \n",
    "    ## Get all the episodes in this season. Similar procedure\n",
    "    episodes_url = base_url + link_name_seas + action\n",
    "    episodes_html = requests.get(episodes_url)\n",
    "    soup_episodes = BeautifulSoup(episodes_html.text, 'html.parser')\n",
    "    \n",
    "    # Get the (one) box of the script links information\n",
    "    main_box_episodes = soup_episodes.find_all('textarea', id='wpTextbox1')[0] # It always has 1 box\n",
    "    episode = 1\n",
    "    matches_episodes = re.findall(pattern , main_box_episodes.get_text())\n",
    "        \n",
    "    for match_episode in matches_episodes:\n",
    "        # Get the link and replace space with _. Not essential, automatically done\n",
    "        link_name_ep = match_episode.replace(' ', '_')\n",
    "                \n",
    "        # Get the name of the episode. Add its number in the beginning \n",
    "        fname = link_name_ep.split('/')[-2].replace('?', '').replace('\\'', '').replace('.', '') # It has script added in the end\n",
    "        fname = fname.replace(':', \"_\")\n",
    "        ep_dir_name = str(episode) + \"_\" + fname + '.txt' \n",
    "        \n",
    "        # Directory to create\n",
    "        episode_dir_path = season_dir_path / ep_dir_name\n",
    "        Path(episode_dir_path).touch()\n",
    "        \n",
    "        \n",
    "        script_link = base_url + link_name_ep\n",
    "        with open(episode_dir_path.as_posix(), 'w', encoding='utf-8') as f:\n",
    "            f.write(get_txt_script(script_link.replace('?', '%3F')))\n",
    "        \n",
    "        \n",
    "        # Increment episode\n",
    "        episode += 1\n",
    "        \n",
    "    \n",
    "        # Update dict\n",
    "        scripts_dict[season] = {episode: script_link}\n",
    "    \n",
    "    # Increment season number\n",
    "    season += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0723d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
